""" Search task implementations. """
import gzip
import os
import queue
import time
import uuid
from functools import cached_property
from collections import UserDict, UserList

import psutil
from searchkit.log import log
from searchkit.result import (
    SearchResult,
    SequenceSearchResults,
)
from searchkit.exception import FileSearchException
from searchkit.searchdef import SequenceSearchDef

RESULTS_QUEUE_TIMEOUT = 60
MAX_QUEUE_RETRIES = 10
RESULTS_QUEUE_SIZE = 100000
NUM_BUFFERED_RESULTS = int(RESULTS_QUEUE_SIZE / 10) or 1


class SearchTaskError(Exception):
    """ General search task error. """


class SearchTaskResultsManager():
    """
    Holds the objects needed to manage results generated by SearchTasks.
    """
    def __init__(self, results_store, results_queue=None,
                 results_collection=None):
        """
        @param results_store: ResultStoreSimple or ResultStoreParallel object
        @param results_queue: optional multiprocessing.Queue. This must be
                              provided if task is running in a child process.
        @param results_collection: optional SearchResultsCollection. This must
                                   only be provided if the task is running in
                                   single process mode i.e. if the search task
                                   is executing in the same same process as the
                                   collector.
        """
        if results_queue is not None and results_collection is not None:
            raise SearchTaskError("only one of results_queue and "
                                  "results_collection can be used with a "
                                  "SearchTask.")

        self._results_store = results_store
        self._results_queue = results_queue
        self._results_collection = results_collection

    @property
    def results_store(self):
        return self._results_store

    @property
    def results_queue(self):
        return self._results_queue

    @property
    def results_collection(self):
        return self._results_collection


class QueueTransitBuffer(UserList):
    """ A variable length buffer for the transfer of results back to the main
    collector process."""
    MAX = 10


class SearchTask():  # pylint: disable=too-many-instance-attributes
    """ Search task implementation for all searches. """
    def __init__(self, info, constraints_manager, results_manager,
                 decode_errors=None):
        """
        Run search task on file.

        @param info: dictionary containing information about what we are
                     searching incl. path, searchdefs etc.
        @param constraints_manager: SearchConstraintsManager object
        @param results_manager: SearchTaskResultsManager object
        @param decode_errors: unicode decode error handling.
        """
        self.proc = None
        self.info = info
        self.stats = SearchTaskStats()
        self.constraints_manager = constraints_manager
        self.results_manager = results_manager
        self.decode_kwargs = {}
        if decode_errors:
            self.decode_kwargs['errors'] = decode_errors
        self.results_buffer = []

    @cached_property
    def id(self):
        return str(uuid.uuid4())

    @cached_property
    def search_defs_conditional(self):
        return [s_def for s_def in self.info['searches']
                if s_def.constraints]

    @cached_property
    def search_defs(self):
        alldefs = {s_def: True for s_def in self.info['searches']}
        for s_def in alldefs:
            if s_def in self.search_defs_conditional:
                alldefs[s_def] = False

        return alldefs

    def put_result(self, results):
        self.stats['results'] += len(results)
        if self.results_manager.results_collection is not None:
            self.results_manager.results_collection.add(results)
            return

        backoff = 1
        max_tries = MAX_QUEUE_RETRIES
        while max_tries > 0:
            try:
                if max_tries == MAX_QUEUE_RETRIES:
                    self.results_manager.results_queue.put_nowait(results)
                else:
                    self.results_manager.results_queue.put(
                        results,
                        timeout=RESULTS_QUEUE_TIMEOUT)

                break
            except queue.Full:
                if max_tries == MAX_QUEUE_RETRIES:
                    msg = ("result queue for task '%s' is full (size=%s) - "
                           "switching to blocking put with timeout "
                           "(retries=%s, backoff=%ss)")
                    log.info(msg, self.info['path'], RESULTS_QUEUE_SIZE,
                             max_tries, backoff)
                else:
                    msg = ("result queue for task '%s' is still full even "
                           "after waiting %ss - trying again "
                           "(retries=%s, backoff=%ss)")
                    log.warning(msg, self.info['path'], RESULTS_QUEUE_TIMEOUT,
                                max_tries, backoff)

                max_tries -= 1
                # backoff for short while to allow queue contents to be
                # consumed.
                time.sleep(backoff)
                if backoff < RESULTS_QUEUE_TIMEOUT:
                    backoff *= 2

        if max_tries == 0:
            log.error("exceeded max number of retries (%s) to put results "
                      "data on the queue", MAX_QUEUE_RETRIES)

    def show_mem_usage(self, label):
        if self.proc is None:
            self.proc = psutil.Process()
        log.debug('%s (rss=%sM)', label,
                  int(self.proc.memory_info().rss / 1024 ** 2))

    def _flush_results_buffer(self):
        # log.debug("flushing results buffer (%s)", len(self.results_buffer))
        # self.show_mem_usage('before')
        limit = QueueTransitBuffer.MAX
        while self.results_buffer:
            try:
                buffr = QueueTransitBuffer(self.results_buffer[:limit])
                self.put_result(buffr)
                for _ in range(limit):
                    self.results_buffer.pop(0)

            except IndexError:
                limit -= 1

        # self.show_mem_usage('after')

    def _simple_search(self, search_def, line, ln):
        """ Perform a simple search on line.

        @param search_def: SearchDef object
        @param line: current line (string)
        @param ln: current line number
        """
        ret = search_def.run(line)
        if not ret:
            return

        result = SearchResult(ln, self.info['source_id'], ret, search_def,
                              self.results_manager.results_store)
        self.results_buffer.append(result.export)
        if len(self.results_buffer) >= NUM_BUFFERED_RESULTS:
            self._flush_results_buffer()

    def _sequence_search(self, seq_def, line, ln, sequence_results):
        """ Perform a sequence search on line.

        @param seq_def: SequenceSearchDef object
        @param line: current line (string)
        @param ln: current line number
        @param sequence_results: SequenceSearchResults object
        """
        ret = seq_def.s_start.run(line)
        # if the ending is defined and we match a start while
        # already in a section, we start again.
        if seq_def.s_end and seq_def.started:
            if ret:
                # reset and start again
                sequence_results.remove(seq_def.id)
                seq_def.reset()
            else:
                ret = seq_def.s_end.run(line)

        if ret:
            if not seq_def.started:
                s_term = seq_def.s_start
                seq_def.start()
                section_id = seq_def.current_section_id
            else:
                s_term = seq_def.s_end
                section_id = seq_def.current_section_id
                seq_def.stop()
                # if no end is defined then we don't bother storing
                # the result, just complete the section and start
                # the next.
                if seq_def.s_end is None:
                    s_term = seq_def.s_start
                    seq_def.start()
                    section_id = seq_def.current_section_id

            sequence_results.add(SearchResult(
                                     ln, self.info['source_id'], ret,
                                     s_term,
                                     self.results_manager.results_store,
                                     sequence_section_id=section_id))
        elif seq_def.started and seq_def.s_body:
            section_id = seq_def.current_section_id
            ret = seq_def.s_body.run(line)
            if ret:
                sequence_results.add(SearchResult(
                                         ln, self.info['source_id'],
                                         ret, seq_def.s_body,
                                         self.results_manager.results_store,
                                         sequence_section_id=section_id))

    def _process_sequence_results(self, sequence_results, current_ln):  # noqa,pylint: disable=too-many-branches
        """
        Perform post processing to sequence search results.

        @param sequence_results: SequenceSearchResults object
        @param current_ln: number of the last line to be read from file
        """
        # If a sequence ending definition is provided and we reached EOF
        # while a sequence is started, complete the sequence if s_end
        # matches an empty string. If s_end is not defined we just go ahead
        # and complete the section.
        filter_section_id = {}
        for s_def in self.search_defs:
            if not isinstance(s_def, SequenceSearchDef):
                continue

            seq_def = s_def
            if not seq_def.started:
                continue

            if seq_def.s_end is None:
                continue

            ret = seq_def.s_end.run('')
            if ret:
                section_id = seq_def.current_section_id
                r = SearchResult(current_ln + 1, self.info['source_id'], ret,
                                 seq_def.s_end,
                                 self.results_manager.results_store,
                                 sequence_section_id=section_id)
                sequence_results.add(r)
            else:
                if seq_def.id not in filter_section_id:
                    filter_section_id[seq_def.id] = []

                filter_section_id[seq_def.id].append(
                    seq_def.current_section_id)

        if len(sequence_results) < 1:
            log.debug("no sequence results to process")
            return

        log.debug("filtering sections: %s", filter_section_id)
        # Now add sequence results to main results list, excluding any
        # incomplete sections.
        for s_results in sequence_results.values():
            for r in s_results:
                if filter_section_id:
                    if r.sequence_id is None:
                        continue

                    seq_id = r.sequence_id
                    if seq_id in filter_section_id:
                        if r.section_id in filter_section_id[seq_id]:
                            continue

                self.results_buffer.append(r.export)
                if len(self.results_buffer) >= NUM_BUFFERED_RESULTS:
                    self._flush_results_buffer()

    def _run_search(self, fd):
        """
        @param fd: open file descriptor
        """
        self.stats.reset()
        sequence_results = SequenceSearchResults()
        search_ids = (s.id for s in self.search_defs)
        offset = self.constraints_manager.apply_global(search_ids, fd)
        log.debug("starting search of %s (offset=%s, pos=%s)", fd.name, offset,
                  fd.tell())
        runnable = {s.id: _runnable
                    for s, _runnable in self.search_defs.items()}
        ln = 0
        # NOTE: line numbers start at 1 hence offset + 1
        for ln, line in enumerate(fd, start=1):
            # This could be helpful to show progress for large files
            if ln % 100000 == 0:
                self.show_mem_usage(f"{ln} lines searched in {fd.name}")

            self.stats['lines_searched'] += 1
            line = line.decode("utf-8", **self.decode_kwargs)

            for s_def in self.search_defs:
                if not runnable[s_def.id]:
                    ret = self.constraints_manager.apply_single(s_def, line)
                    if not ret.line_is_valid:
                        continue

                    # enable from here on in if *all* constraints passed
                    runnable[s_def.id] = ret.all_constraints_passed

                if isinstance(s_def, SequenceSearchDef):
                    self._sequence_search(s_def, line, ln, sequence_results)
                else:
                    self._simple_search(s_def, line, ln)

        self._process_sequence_results(sequence_results, ln)
        # allow garbage collect
        sequence_results = {}

        log.debug("completed search of %s lines", self.stats['lines_searched'])
        if self.search_defs_conditional:
            msg = f"constraints stats {fd.name}:"
            for sd in self.search_defs_conditional:
                if sd.constraints:
                    for c in sd.constraints.values():
                        msg += f"\n  id={c.id}: {c.stats()}"

            log.debug(msg)

        log.debug("run search complete for path %s", fd.name)
        return self.stats

    def failed(self, exc):
        """ This should be called if the task failed to execute. """
        log.error("search task failed for path=%s with exception %s",
                  self.info['path'], exc)

    def execute(self):
        stats = SearchTaskStats()
        path = self.info['path']
        if os.path.getsize(path) == 0:
            log.debug("filesearcher: zero-length file %s - skipping search",
                      path)
            return stats

        log.debug("starting execution on path %s (searches=%s)", path,
                  len(self.search_defs))
        try:
            # first assume compressed then plain
            with gzip.open(path, 'rb') as fd:
                try:
                    # test if file is gzip
                    fd.peek(1)
                except OSError:
                    with open(path, 'rb') as fd:
                        stats = self._run_search(fd)
                else:
                    stats = self._run_search(fd)
                finally:
                    self._flush_results_buffer()
        except UnicodeDecodeError:
            log.exception("caught UnicodeDecodeError while searching %s", path)
            raise
        except EOFError as e:
            log.exception("")
            msg = f"an exception occurred while searching {path} - {e}"
            raise FileSearchException(msg) from e
        except Exception as e:
            log.exception("")
            msg = (f"an unexpected exception occurred while searching {path} "
                   f"- {e}")
            raise FileSearchException(msg) from e

        self.results_manager.results_store.sync()
        log.debug("finished execution on path %s", path)
        return stats


class SearchTaskStats(UserDict):
    """ Keep stats on search tasks executed. """
    def __init__(self):
        super().__init__()
        self.reset()

    def reset(self):
        self.data = {'searches': 0,
                     'searches_by_job': [],
                     'lines_searched': 0,
                     'jobs_completed': 0,
                     'total_jobs': 0,
                     'results': 0,
                     'parts_deduped': 0,
                     'parts_non_deduped': 0}

    def update(self, stats):  # pylint: disable=arguments-differ
        if not stats:
            return

        for key, val in stats.items():
            self.data[key] += val

    def __repr__(self):
        return ', '.join([f"{k}={v}" for k, v in self.data.items()])
