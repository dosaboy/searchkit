import abc
import concurrent.futures
import copy
import glob
import gzip
import multiprocessing
import os
import queue
import re
import signal
import subprocess
import threading
import time
import uuid
from functools import cached_property
from collections import namedtuple, UserDict, UserList

from searchkit.log import log
from searchkit.constraints import CouldNotApplyConstraint

RESULTS_QUEUE_TIMEOUT = 60
MAX_QUEUE_RETRIES = 10
RS_LOCK = multiprocessing.Lock()
NUM_BUFFERED_RESULTS = 100


def rs_locked(f):
    def _rs_locked_inner(*args, **kwargs):
        with RS_LOCK:
            return f(*args, **kwargs)

    return _rs_locked_inner


class FileSearchException(Exception):
    def __init__(self, msg):
        self.msg = msg


class SearchDefBase(object):

    def __init__(self, constraints=None):
        """
        @param constraints: an optional list of constraints to apply to
                            results.
        """
        self.id
        self.constraints = {c.id: c for c in constraints or {}}

    @cached_property
    def id(self):
        """ A unique identifier for this search definition. """
        return str(uuid.uuid4())


class SearchDef(SearchDefBase):

    def __init__(self, pattern, tag=None, hint=None,
                 store_result_contents=True, field_info=None, **kwargs):
        """
        Simple search definition.

        @param pattern: pattern or list of patterns to search for
        @param tag: optional user-friendly identifier for this search term.
                    This is useful for retrieving results.
        @param hint: optional pre-search term. If provided, this is expected to
                     match in order for the main search to be executed.
        @param store_result_contents: by default the content of a search result
                                      is saved but if it is not needed this
                                      can be set to False. This effectively
                                      makes the result True/False.
        @param field_info: optional ResultFieldInfo object
        """
        if not isinstance(pattern, list):
            self.patterns = [re.compile(pattern)]
        else:
            self.patterns = []
            for _pattern in pattern:
                self.patterns.append(re.compile(_pattern))

        self.store_result_contents = store_result_contents
        self.tag = tag
        self.field_info = field_info
        self.hint = hint
        if hint:
            self.hint = re.compile(hint)

        self.sequence_def = None

        # do this last
        super().__init__(**kwargs)

    def link_to_sequence(self, sequence_def, tag):
        """
        If this search definition is part of a sequence, the parent
        SequenceSearchDef must link itself to this object.

        @param sequence_def: SequenceSearchDef object
        @param tag: SequenceSearchDef object tag for this section def
        """
        self.sequence_def = sequence_def
        self.tag = tag

    def run(self, line):
        """ Execute search patterns against line and return first match. """
        if self.hint:
            ret = self.hint.search(line)
            if not ret:
                return None

        ret = None
        for pattern in self.patterns:
            ret = pattern.match(line)
            if ret:
                break

        return ret

    def __repr__(self):
        return ', '.join([str(p) for p in self.patterns])


class SequenceSearchDef(SearchDefBase):

    def __init__(self, start, tag, end=None, body=None, **kwargs):
        """
        Sequence search definition.

        A sequence must match a start and end with optional body in between.
        If no end is defined, the sequence ends with the start of the next or
        EOF.

        NOTE: sequences must not overlap. This is therefore not suitable for
        finding sequences generated by parallel/concurrent tasks.

        @param start: SearchDef object for matching start
        @param tag: tag used to identify this sequence definition
        @param end: optional SearchDef object for matching end
        @param body: optional SearchDef object for matching body
        """
        self.tag = tag
        self.s_start = start
        self.s_end = end
        self.s_body = body

        # make sure section defs have tags synced with this object
        for sd, _tag in {start: self.start_tag,
                         end: self.end_tag,
                         body: self.body_tag}.items():
            if sd:
                sd.link_to_sequence(self, _tag)

        self._mark = None
        # Each section matched gets its own id. Since each file is processed
        # using a separate process and memory is not shared, these values must
        # be unique to avoid collisions when results are aggregated.
        self._section_id = None
        self.completed_sections = []
        # do this last
        super().__init__(**kwargs)

    @property
    def start_tag(self):
        """ Tag used to identify start of section. """
        return "{}-start".format(self.tag)

    @property
    def end_tag(self):
        """ Tag used to identify end of section. """
        return "{}-end".format(self.tag)

    @property
    def body_tag(self):
        """ Tag used to identify body of section. """
        return "{}-body".format(self.tag)

    @property
    def current_section_id(self):
        """ ID of current section. A new id should be set after each
        completed section. """
        return self._section_id

    @property
    def started(self):
        """ Indicate a section sequence has been started. """
        return self._mark == 1

    def start(self):
        """ Indicate that a sequence start has been detected. """
        self._section_id = str(uuid.uuid4())
        log.debug("sequence %s started section %s (completed=%s)",
                  self.id, self.current_section_id,
                  len(self.completed_sections))
        self._mark = 1

    def reset(self):
        """ Used to restart a section. This is used e.g. if the start
        expression matches midway through a sequence (and before the end).
        """
        self._mark = 0

    def stop(self):
        """ Indicate that a sequence is complete. """
        self._mark = 0
        if self.current_section_id is None:
            raise FileSearchException("sequence section id is None")

        self.completed_sections.append(self.current_section_id)
        log.debug("sequence %s stopping section %s (completed=%s)",
                  self.id, self.current_section_id,
                  len(self.completed_sections))
        self._section_id = str(uuid.uuid4())

    def __repr__(self):
        return ("{}: current_section={}, started={}, completed_sections={}".
                format(self.__class__.__name__, self.current_section_id,
                       self.started, self.completed_sections))


class SequenceSearchResults(UserDict):

    def __init__(self):
        self.data = {}

    def add(self, result):
        sid = result.sequence_id
        if sid in self.data:
            self.data[sid].append(result)
        else:
            self.data[sid] = [result]

    def remove(self, sid):
        if sid in self.data:
            del self.data[sid]


class ResultStoreBase(UserList):
    """
    This class is used to de-duplicate values stored in search results such
    that allowing their reference to be saved in the result for later lookup.
    """

    def __init__(self):
        self.counters = {}
        self.value_store = self.data = []
        self.tag_store = []
        self.sequence_id_store = []

    def __getitem__(self, result_id):
        if result_id >= len(self.value_store):
            return None

        return self.value_store[result_id]

    @property
    def parts_deduped(self):
        counters = self.counters.values()
        return sum(counters) - len(counters)

    @property
    def parts_non_deduped(self):
        return len(self.value_store)

    def _get_value_index(self, value, store):
        """
        If the value is not None and not already saved, save it.

        Returns the position of the value in the store or -1 if it is None.
        """
        if value is None:
            return -1

        if value in store:
            return store.index(value)

        store.append(value)
        return len(store) - 1

    def add(self, tag, sequence_id, value):
        """
        Ensure the given values are saved in the store and return their
        position.

        Returns a tuple of references to the position in the store of each
        value. A ref value of -1 indicates that the value does not exist and is
        not stored.

        @param tag: optional search tag
        @param sequence_id: optional sequence search id
        @param value: search result value
        """
        value_idx = self._get_value_index(value, self.value_store)
        if value_idx >= 0:
            # increment global counter
            if value_idx not in self.counters:
                self.counters[value_idx] = 1
            else:
                self.counters[value_idx] += 1

        tag_idx = self._get_value_index(tag, self.tag_store)
        sequence_id_idx = self._get_value_index(sequence_id,
                                                self.sequence_id_store)
        return tag_idx, sequence_id_idx, value_idx


class ResultStoreSimple(ResultStoreBase):
    """ Store for use when sharing between processes is not needed. """


class ResultStoreParallel(ResultStoreBase):
    """ Store for use when sharing between processes is required. """

    def __init__(self, mgr):
        self.counters = mgr.dict()
        self.value_store = self.data = mgr.list()
        self.tag_store = mgr.list()
        self.sequence_id_store = mgr.list()

    @rs_locked
    def __getitem__(self, result_id):
        return super().__getitem__(result_id)

    @rs_locked
    def add(self, *args, **kwargs):
        return super().add(*args, **kwargs)

    @property
    @rs_locked
    def parts_deduped(self):
        return super().parts_deduped

    @property
    @rs_locked
    def parts_non_deduped(self):
        return super().parts_non_deduped

    @rs_locked
    def unproxy_results(self):
        """
        Converts internal stores to unproxied types so they can be accessed
        once their manager is gone.
        """
        log.debug("unproxying results store (data=%s)", len(self.value_store))
        self.value_store = self.data = copy.deepcopy(self.data)
        self.tag_store = copy.deepcopy(self.tag_store)
        self.sequence_id_store = copy.deepcopy(self.sequence_id_store)
        self.counters = self.counters.copy()


class ResultFieldInfo(UserDict):

    def __init__(self, fields):
        """
        @param fields: list or dictionary of field names. If a dictionary is
                       provided, the values are expected to be functions that
                       the field value will be cast to. In other words these
                       should typically be standard or custom types.
        """
        if issubclass(fields.__class__, dict):
            self.data = fields
        else:
            self.data = {f: None for f in fields}

    def ensure_type(self, name, value):
        """
        If our fields have associated type functions, cast the value to
        its expected type.
        """
        if name not in self.data or self.data[name] is None:
            return value

        return self.data[name](value)

    def index_to_name(self, index):
        """ Retrieve a field name using the result group index. """
        for i, _field in enumerate(self.data):
            if index == i:
                return _field

        raise FileSearchException("field with index {} not found in mapping".
                                  format(index))


class SearchResultBase(UserList):
    META_OFFSET_TAG = 0
    META_OFFSET_SEQ_ID = 1

    PART_OFFSET_IDX = 0
    PART_OFFSET_VALUE = 1
    PART_OFFSET_FIELD = 2

    @abc.abstractmethod
    def __init__(self):
        self.results_store = None
        self.linenumber = None
        self.section_id = None

    def _get_store_id(self, field):
        for part in self.data:
            store_id = None
            # Entry has format: (<idx>, <tag_id>, <store id>, <field name>)
            if len(part) > self.PART_OFFSET_FIELD and isinstance(field, str):
                if part[self.PART_OFFSET_FIELD] != field:
                    continue
            elif part[self.PART_OFFSET_IDX] != field:
                continue

            store_id = part[self.PART_OFFSET_VALUE]
            if store_id is not None:
                return store_id

    def get(self, field):
        """
        Retrieve result part value by index or name.

        @param field: integer index of string field name.
        """
        store_id = self._get_store_id(field)
        if store_id is not None:
            return self.results_store[store_id]

    def __iter__(self):
        """ Only return part values when iterating over this object. """
        for part in self.data:
            yield self.results_store[part[self.PART_OFFSET_VALUE]]

    def __repr__(self):
        r_list = ["{}='{}'".
                  format(rp[self.PART_OFFSET_IDX],
                         self.results_store[rp[self.PART_OFFSET_VALUE]])
                  for rp in self.data]
        return ("ln:{} {} (section={})".
                format(self.linenumber, ", ".join(r_list), self.section_id))


class SearchResultMinimal(SearchResultBase):

    def __init__(self, data, metadata, linenumber, source_id,
                 sequence_section_id, field_info):
        """
        This is a minimised representation of a SearchResult object so as to
        reduce its size as much as possible before putting on the results
        queue.

        IMPORTANT: this class must contain as few attributes as possible and
        their values must be as small as possible. For large values that need
        to be shared, we de-duplicate using ResultStoreBase implementations.

        Do not store references to shared objects.
        """
        self.data = data[:]
        self.metadata = metadata[:]
        self.linenumber = linenumber
        self.source_id = source_id
        self.section_id = sequence_section_id
        if field_info:
            self.field_names = list(field_info)
        else:
            self.field_names = None

        self.results_store = None

    def __getattr__(self, name):
        if name != 'field_names':
            if self.field_names and name in self.field_names:
                return self.get(name)

        raise AttributeError("'{}' object has no attribute '{}'".
                             format(self.__class__.__name__, name))

    @property
    def tag(self):
        idx = self.metadata[self.META_OFFSET_TAG]
        if idx < 0:
            return

        return self.results_store.tag_store[idx]

    @property
    def sequence_id(self):
        idx = self.metadata[self.META_OFFSET_SEQ_ID]
        if idx < 0:
            return

        return self.results_store.sequence_id_store[idx]

    def register_results_store(self, store):
        """
        Register a ResultsStore with this result. This is used to re-register
        the store once the result has been received by the main process.

        @param store: ResultsStore object
        """
        self.results_store = store


class SearchResult(SearchResultBase):

    def __init__(self, linenumber, source_id, result, search_def,
                 results_store, sequence_section_id=None):
        """
        @param linenumber: line number that produced a match.
        @param source_id: data source id - resolves to a path in the
                          SearchCatalog.
        @param result: python.re.match object.
        @param search_def: SearchDef object.
        @param results_store: ResultsStore object
        @param sequence_section_id: if this result is part of a sequence the
                                    section ID must be provided.
        """
        self.results_store = results_store
        self.data = []
        self.linenumber = linenumber
        self.source_id = source_id
        self.tag = search_def.tag
        self.section_id = sequence_section_id
        self.sequence_id = None
        if search_def.sequence_def:
            if sequence_section_id is None:
                raise FileSearchException("sequence section result saved "
                                          "but no section id provided")

            self.sequence_id = search_def.sequence_def.id

        self.field_info = search_def.field_info

        if not search_def.store_result_contents:
            log.debug("store_contents is False - skipping save value")
            return

        self.store_result(result)

    def store_result(self, result):
        num_groups = len(result.groups())
        # NOTE: this does not include group(0)
        if num_groups:
            # To reduce memory footprint, don't store group(0) i.e. the whole
            # line, if there are actual groups in the result.
            for i in range(1, num_groups + 1):
                self._save_part(i, result.group(i))
        else:
            log.debug("Saving full search result is inefficient and can lead "
                      "to high memory usage. Use sub groups if possible. "
                      "(tag=%s)", self.tag)
            self._save_part(0, result.group(0))

    @property
    def metadata(self):
        tag_id, seq_id, _ = self.results_store.add(self.tag, self.sequence_id,
                                                   value=None)
        return (tag_id, seq_id)

    def _save_part(self, part_index, value=None):
        name = None
        if value is not None:
            if self.field_info:
                name = self.field_info.index_to_name(part_index - 1)
                value = self.field_info.ensure_type(name, value)

        _, _, store_id = self.results_store.add(self.tag, self.sequence_id,
                                                value)
        if name is None:
            entry = (part_index, store_id)
        else:
            entry = (part_index, store_id, name)

        self.data.append(entry)

    @cached_property
    def export(self):
        """ Export the smallest possible representation of this object. """
        return SearchResultMinimal(self.data, self.metadata,
                                   self.linenumber, self.source_id,
                                   self.section_id, self.field_info)


class SearchResultsCollection(UserDict):

    def __init__(self, search_catalog, results_store):
        self.search_catalog = search_catalog
        self.results_store = results_store
        self.reset()

    @property
    def data(self):
        results = {}
        for path, _results in self._results_by_path.items():
            results[path] = _results

        return results

    @property
    def all(self):
        for results in self._results_by_path.values():
            for r in results:
                yield r

    def reset(self):
        self._results_by_path = {}

    @property
    def files(self):
        return list(self._results_by_path.keys())

    def add(self, result):
        result.register_results_store(self.results_store)
        # resolve
        path = self.search_catalog.source_id_to_path(result.source_id)
        if path not in self._results_by_path:
            self._results_by_path[path] = [result]
        else:
            self._results_by_path[path].append(result)

    def find_by_path(self, path):
        """ Return results for a given path. """
        return self._results_by_path.get(path, [])

    def find_by_tag(self, tag, path=None):
        """ Return results matched by tag.

        @param tag: tag used to identify search results.
        @param path: optional path used to filter results to only include those
                     matched from a given path.
        """
        if path:
            paths = [path]
        else:
            paths = list(self._results_by_path.keys())

        results = []
        for _path in paths:
            for result in self.find_by_path(_path):
                if result.tag != tag:
                    continue

                results.append(result)

        return results

    def _get_all_sequence_results(self, path=None):
        """ Return a list of ids for all sequence match results.

        @param path: optionally filter results for a given path.
        """
        if path:
            paths = [path]
        else:
            paths = list(self._results_by_path.keys())

        sequences = []
        for _path in paths:
            for result in self.find_by_path(_path):
                if result.sequence_id is None:
                    continue

                sequences.append(result)

        return sequences

    def find_sequence_by_tag(self, tag, path=None):
        """ Find results for the sequence search(es) identified from tag.

        Returns a dictionary of "sections" i.e. complete sequences matched
        using associated SequenceSearchDef objects. Each section is a list of
        SearchResult objects representing start/body/end for that section.

        @param tag: tag used to identify sequence results.
        @param path: optionally filter results for a given path.
        """
        sections = {}
        for seq_obj in self.search_catalog.resolve_from_tag(tag):
            sections.update(self.find_sequence_sections(seq_obj, path))

        return sections

    def find_sequence_sections(self, sequence_obj, path=None):
        """ Find results for the given sequence search.

        Returns a dictionary of "sections" i.e. complete sequences matched
        using the associated SequenceSearchDef object. Each section is a list
        of SearchResult objects representing start/body/end for that section.

        @param sequence_obj: SequenceSearch object
        @param path: optionally filter results for a given path.
        """
        _results = {}
        for result in self._get_all_sequence_results(path=path):
            s_id = result.sequence_id
            if s_id != sequence_obj.id:
                continue

            section_id = result.section_id
            if section_id not in _results:
                _results[section_id] = []

            _results[section_id].append(result)

        return _results

    def __len__(self):
        """ Returns total number of search results. """
        _count = 0
        for f in self.files:
            _count += len(self.find_by_path(f))

        return _count


class LogrotateLogSort(object):

    def __call__(self, fname):
        """
        Sort contents of a directory by passing the function as the key to a
        list sort. Directory is expected to contain logfiles with extensions
        used by logrotate e.g. .log, .log.1, .log.2.gz etc.
        """
        filters = [r"\S+\.log$",
                   r"\S+\.log\.(\d+)$",
                   r"\S+\.log\.(\d+)\.gz?$"]
        for f in filters:
            ret = re.compile(f).match(fname)
            if ret:
                break

        # files that don't follow logrotate naming format go to the end.
        if not ret:
            # put at the end
            return 100000

        if len(ret.groups()) == 0:
            return 0

        return int(ret.group(1))


class SearchCatalog(object):

    def __init__(self, max_logrotate_depth=7):
        self.max_logrotate_depth = max_logrotate_depth
        self._source_ids = {}
        self._search_tags = {}
        self._simple_searches = {}
        self._sequence_searches = {}
        self._entries = {}

    def register(self, search, user_path):
        """
        Register a search against a path.

        The same search can be registered against more than one path.

        @param search: object implemented from SearchDefBase.
        @param user_path: directory or file path.
        """
        if search.tag is not None:
            if search.tag in self._search_tags:
                if search.id not in self._search_tags[search.tag]:
                    log.debug("one or more search tagged '%s' has already "
                              "been registered against path '%s'",
                              search.tag, user_path)
                    self._search_tags[search.tag].append(search.id)
            else:
                self._search_tags[search.tag] = [search.id]

        if isinstance(search, SequenceSearchDef):
            self._sequence_searches[search.id] = search
        else:
            self._simple_searches[search.id] = search

        for path in self._expand_path(user_path):
            if path in self._entries:
                entry = self._entries[path]
                entry['searches'].append(search)
            else:
                self._entries[path] = {'source_id': self._get_source_id(path),
                                       'path': path,
                                       'searches': [search]}

    def resolve_from_id(self, search_id):
        """ Resolve search definition from unique id. """
        if search_id in self._simple_searches:
            return self._simple_searches[search_id]

        return self._sequence_searches[search_id]

    def resolve_from_tag(self, tag):
        """ Resolve search definition from tag.

        Returns a list of resolved searches.
        """
        searches = []
        for search_id in self._search_tags[tag]:
            searches.append(self.resolve_from_id(search_id))

        return searches

    def _filtered_dir(self, contents, max_logrotate_depth=7):
        """ Filter contents of a directory. Directories are ignored and if any
        files look like logrotated log files they are sorted and only
        max_logrotate_depth are kept.
        """
        logrotated = {}
        new_contents = []
        for path in contents:
            if not os.path.isfile(path):
                continue

            ret = re.compile(r"(\S+)\.log\S*").match(path)
            if not ret:
                new_contents.append(path)
                continue

            fnamepfix = ret.group(1)
            if path.endswith('.log'):
                new_contents.append(fnamepfix + '.log')
            else:
                if fnamepfix not in logrotated:
                    logrotated[fnamepfix] = [path]
                else:
                    logrotated[fnamepfix].append(path)

        limit = max_logrotate_depth
        for logrotated in logrotated.values():
            capped = sorted(logrotated,
                            key=LogrotateLogSort())[:limit]
            new_contents += capped

        return new_contents

    def _expand_path(self, path):
        if os.path.isfile(path):
            return [path]

        if os.path.isdir(path):
            return self._filtered_dir(os.listdir(path),
                                      self.max_logrotate_depth)

        return self._filtered_dir(glob.glob(path), self.max_logrotate_depth)

    def source_id_to_path(self, s_id):
        try:
            return self._source_ids[s_id]
        except KeyError:
            log.exception("ALL PATHS:")
            log.error('\n'.join(list(self._source_ids.keys())))

    def _get_source_id(self, path):
        if not self._source_ids:
            source_id = 0
        else:
            for source_id, _path in self._source_ids.items():
                if _path == path:
                    return source_id

            source_id = max(list(self._source_ids)) + 1

        log.debug("path=%s source_id=%s", path, source_id)
        self._source_ids[source_id] = path
        return source_id

    def __len__(self):
        return len(self._entries)

    def __iter__(self):
        for entry in self._entries.values():
            yield entry

    def __repr__(self):
        info = ""
        for path, searches in self._entries.items():
            info += "\n{}:\n    ".format(path)
            entries = []
            for key, val in searches.items():
                entries.append("{}={}".format(key, val))

            info += '\n    '.join(entries)

        return info


class SearchTask(object):

    def __init__(self, info, constraints_manager, results_store,
                 results_queue=None, results_collection=None,
                 decode_errors=None):
        """
        Run search task on file.

        @param info: dictionary containing information about what we are
                     searching incl. path, searchdefs etc.
        @param constraints_manager: SearchConstraintsManager object
        @param results_store: ResultStoreSimple or ResultStoreParallel object
        @param results_queue: optional multiprocessing.Queue. This must be
                              provided if task is running in a child process.
        @param results_collection: optional SearchResultsCollection. This must
                                  be provided if task is not running in
                                  parallel mode.
        @param decode_errors: unicode decode error handling.
        """
        if results_queue is not None and results_collection is not None:
            raise Exception("only one of results_queue and results_collection "
                            "can be used with a SearchTask.")

        self.info = info
        self.stats = SearchTaskStats()
        self.constraints_manager = constraints_manager
        self.results_queue = results_queue
        self.results_collection = results_collection
        self.results_store = results_store
        self.decode_kwargs = {}
        if decode_errors:
            self.decode_kwargs['errors'] = decode_errors
        self.buffered_results = []

    @cached_property
    def id(self):
        return str(uuid.uuid4())

    @cached_property
    def search_defs_conditional(self):
        return [s_def for s_def in self.info['searches']
                if s_def.constraints]

    @cached_property
    def search_defs(self):
        alldefs = {s_def: True for s_def in self.info['searches']}
        for s_def in alldefs:
            if s_def in self.search_defs_conditional:
                alldefs[s_def] = False

        return alldefs

    def put_result(self, result):
        self.stats['results'] += 1
        if self.results_collection is not None:
            self.results_collection.add(result)
            return

        max_tries = MAX_QUEUE_RETRIES
        while max_tries > 0:
            try:
                if max_tries == MAX_QUEUE_RETRIES:
                    self.results_queue.put_nowait(result)
                else:
                    self.results_queue.put(result,
                                           timeout=RESULTS_QUEUE_TIMEOUT)

                break
            except queue.Full:
                if max_tries == MAX_QUEUE_RETRIES:
                    msg = ("search task queue for '%s' is full - switching "
                           "to using blocking put with timeout")
                    log.info(msg, self.info['path'])
                else:
                    msg = ("search task queue for '%s' is full even after "
                           "waiting %ss - trying again")
                    log.warning(msg, self.info['path'],
                                RESULTS_QUEUE_TIMEOUT)

                max_tries -= 1

        if max_tries == 0:
            log.error("exceeded max number of retries (%s) to put results "
                      "data on the queue", MAX_QUEUE_RETRIES)

    def _flush_results_buffer(self):
        # log.debug("flushing results buffer (%s)", len(self.buffered_results))
        for result in self.buffered_results:
            self.put_result(result)

        self.buffered_results = []

    def _simple_search(self, search_def, line, ln):
        """ Perform a simple search on line.

        @param search_def: SearchDef object
        @param line: current line (string)
        @param ln: current line number
        """
        ret = search_def.run(line)
        if not ret:
            return

        result = SearchResult(ln, self.info['source_id'], ret, search_def,
                              self.results_store)
        self.buffered_results.append(result.export)
        if len(self.buffered_results) >= NUM_BUFFERED_RESULTS:
            self._flush_results_buffer()

    def _sequence_search(self, seq_def, line, ln, sequence_results):
        """ Perform a sequence search on line.

        @param seq_def: SequenceSearchDef object
        @param line: current line (string)
        @param ln: current line number
        @param sequence_results: SequenceSearchResults object
        """
        ret = seq_def.s_start.run(line)
        # if the ending is defined and we match a start while
        # already in a section, we start again.
        if seq_def.s_end and seq_def.started:
            if ret:
                # reset and start again
                sequence_results.remove(seq_def.id)
                seq_def.reset()
            else:
                ret = seq_def.s_end.run(line)

        if ret:
            if not seq_def.started:
                s_term = seq_def.s_start
                seq_def.start()
                section_id = seq_def.current_section_id
            else:
                s_term = seq_def.s_end
                section_id = seq_def.current_section_id
                seq_def.stop()
                # if no end is defined then we don't bother storing
                # the result, just complete the section and start
                # the next.
                if seq_def.s_end is None:
                    s_term = seq_def.s_start
                    seq_def.start()
                    section_id = seq_def.current_section_id

            sequence_results.add(SearchResult(ln, self.info['source_id'], ret,
                                              s_term, self.results_store,
                                              sequence_section_id=section_id))
        elif seq_def.started and seq_def.s_body:
            section_id = seq_def.current_section_id
            ret = seq_def.s_body.run(line)
            if ret:
                sequence_results.add(SearchResult(
                                            ln, self.info['source_id'],
                                            ret, seq_def.s_body,
                                            self.results_store,
                                            sequence_section_id=section_id))

    def _process_sequence_results(self, sequence_results, current_ln):
        """
        Perform post processing to sequence search results.

        @param sequence_results: SequenceSearchResults object
        @param current_ln: number of the last line to be read from file
        """
        # If a sequence ending definition is provided and we reached EOF
        # while a sequence is started, complete the sequence if s_end
        # matches an empty string. If s_end is not defined we just go ahead
        # and complete the section.
        filter_section_id = {}
        for s_def in self.search_defs:
            if not isinstance(s_def, SequenceSearchDef):
                continue

            seq_def = s_def
            if not seq_def.started:
                continue

            if seq_def.s_end is None:
                continue

            ret = seq_def.s_end.run('')
            if ret:
                section_id = seq_def.current_section_id
                r = SearchResult(current_ln + 1, self.info['source_id'], ret,
                                 seq_def.s_end,
                                 self.results_store,
                                 sequence_section_id=section_id)
                sequence_results.add(r)
            else:
                if seq_def.id not in filter_section_id:
                    filter_section_id[seq_def.id] = []

                filter_section_id[seq_def.id].append(
                    seq_def.current_section_id)

        if len(sequence_results) < 1:
            log.debug("no sequence results to process")
            return

        log.debug("filtering sections: %s", filter_section_id)
        # Now add sequence results to main results list, excluding any
        # incomplete sections.
        for s_results in sequence_results.values():
            for r in s_results:
                if filter_section_id:
                    if r.sequence_id is None:
                        continue

                    seq_id = r.sequence_id
                    if seq_id in filter_section_id:
                        if r.section_id in filter_section_id[seq_id]:
                            continue

                self.buffered_results.append(r.export)
                if len(self.buffered_results) >= NUM_BUFFERED_RESULTS:
                    self._flush_results_buffer()

    def _run_search(self, fd):
        """
        @param fd: open file descriptor
        """
        self.stats.reset()
        sequence_results = SequenceSearchResults()
        search_ids = set([s.id for s in self.search_defs])  # noqa, pylint: disable=R1718
        offset = self.constraints_manager.apply_global(search_ids, fd)
        log.debug("starting search of %s (offset=%s, pos=%s)", fd.name, offset,
                  fd.tell())
        runnable = {s.id: _runnable
                    for s, _runnable in self.search_defs.items()}
        ln = 0
        # NOTE: line numbers start at 1 hence offset + 1
        for ln, line in enumerate(fd, start=offset + 1):
            # This could be helpful to show progress for large files
            if ln % 100000 == 0:
                log.debug("%s lines searched in %s", ln, fd.name)

            self.stats['lines_searched'] += 1
            line = line.decode("utf-8", **self.decode_kwargs)

            for s_def in self.search_defs:
                if not runnable[s_def.id]:
                    ret = self.constraints_manager.apply_single(s_def, line)
                    if not ret.line_is_valid:
                        continue

                    # enable from here on in if *all* constraints passed
                    runnable[s_def.id] = ret.all_constraints_passed

                if isinstance(s_def, SequenceSearchDef):
                    self._sequence_search(s_def, line, ln, sequence_results)
                else:
                    self._simple_search(s_def, line, ln)

        self._process_sequence_results(sequence_results, ln)
        # allow garbage collect
        sequence_results = {}

        log.debug("completed search of %s lines", self.stats['lines_searched'])
        if self.search_defs_conditional:
            msg = "constraints stats {}:".format(fd.name)
            for sd in self.search_defs_conditional:
                if sd.constraints:
                    for c in sd.constraints.values():
                        msg += "\n  id={}: {}".format(c.id, c.stats())

            log.debug(msg)

        log.debug("run search complete for path %s", fd.name)
        return self.stats

    def failed(self, exc):
        """ This should be called if the task failed to execute. """
        log.error("search task failed for path=%s with exception %s",
                  self.info['path'], exc)

    def execute(self):
        stats = SearchTaskStats()
        path = self.info['path']
        if os.path.getsize(path) == 0:
            log.debug("filesearcher: zero-length file %s - skipping search",
                      path)
            return stats

        log.debug("starting execution on path %s (searches=%s)", path,
                  len(self.search_defs))
        try:
            # first assume compressed then plain
            with gzip.open(path, 'rb') as fd:
                try:
                    # test if file is gzip
                    fd.read(1)
                    fd.seek(0)
                    stats = self._run_search(fd)
                    self._flush_results_buffer()
                except OSError:
                    with open(path, 'rb') as fd:
                        stats = self._run_search(fd)
                        self._flush_results_buffer()
        except UnicodeDecodeError:
            log.exception("caught UnicodeDecodeError while searching %s", path)
            raise
        except EOFError as e:
            log.exception("")
            msg = ("an exception occurred while searching {} - {}".
                   format(path, e))
            raise FileSearchException(msg) from e
        except Exception as e:
            log.exception("")
            msg = ("an unexpected exception occurred while searching {} - {}".
                   format(path, e))
            raise FileSearchException(msg) from e

        log.debug("finished execution on path %s", path)
        return stats


class SearchTaskStats(UserDict):

    def __init__(self):
        self.reset()

    def reset(self):
        self.data = {'searches': 0,
                     'searches_by_job': [],
                     'lines_searched': 0,
                     'jobs_completed': 0,
                     'total_jobs': 0,
                     'results': 0,
                     'parts_deduped': 0,
                     'parts_non_deduped': 0}

    def update(self, stats):  # pylint: disable=W0221
        if not stats:
            return

        for key, val in stats.items():
            self.data[key] += val

    def __repr__(self):
        return ', '.join([f"{k}={v}" for k, v in self.data.items()])


class SearcherBase(abc.ABC):

    @property
    @abc.abstractmethod
    def files(self):
        """ Returns a list of files we will be searching. """

    @property
    @abc.abstractmethod
    def num_parallel_tasks(self):
        """
        Returns an integer representing the maximum number of tasks we can
        run in parallel. This will typically be bound by the number of
        cpu threads available.
        """

    @abc.abstractmethod
    def add(self, searchdef):
        """
        Add a search criterea.

        @param searchdef: SearchDef object
        """

    @abc.abstractmethod
    def run(self):
        """
        Execute all searches.
        """


class SearchConstraintsManager(object):

    def __init__(self, search_catalog):
        self.search_catalog = search_catalog
        self.global_constraints = []
        self.global_restrictions = set()

    def apply_global(self, search_ids, fd):
        """ Apply any global constraints to the entire file. """
        offset = 0
        if not self.global_constraints:
            log.debug("no global constraint to apply to %s", fd.name)
            return offset

        if self.global_restrictions.intersection(search_ids):
            log.debug("skipping global constraint for %s", fd.name)
            return offset

        for c in self.global_constraints:
            log.debug("applying task global constraint %s to %s", c.id,
                      fd.name)
            _offset = c.apply_to_file(fd)
            if _offset is not None:
                return _offset

        return offset

    def apply_single(self, searchdef, line):
        """ Apply any constraints for this searchdef to the give line.

        @param searchdef: SearchDef object
        @param line: string line we want to validate
        @return: tuple of showing if line passes and if so, did all
                 constraints applied pass.
        """
        Result = namedtuple('result',
                            ('line_is_valid', 'all_constraints_passed'))

        if not searchdef.constraints:
            return Result(True, True)

        any_passed = False
        all_passed = True
        for c in searchdef.constraints.values():
            try:
                if c.apply_to_line(line):
                    any_passed = True
                    continue
            except CouldNotApplyConstraint:
                all_passed = False
                continue

            return Result(False, False)

        return Result(any_passed, all_passed)


class FileSearcher(SearcherBase):

    def __init__(self, max_parallel_tasks=8, max_logrotate_depth=7,
                 constraint=None, decode_errors=None):
        """
        @param max_parallel_tasks: max number of search tasks that can run in
                                   parallel.
        @param max_logrotate_depth: used by SearchCatalog to filter logfiles
                                    based on their name if it matches a
                                    logrotate format and want to constrain how
                                    much history we search.
        @param constraint: constraint to be used with this
                                   searcher that applies to all files searched.
        @param decode_errors: unicode decode error handling. This usually
                              defaults to "strict". See
                              https://docs.python.org/3/howto/unicode.html
                              for more options.
        """
        self.max_parallel_tasks = max_parallel_tasks
        self._stats = SearchTaskStats()
        self.catalog = SearchCatalog(max_logrotate_depth)
        self.constraints_manager = SearchConstraintsManager(self.catalog)
        self.decode_errors = decode_errors
        if constraint:
            self.constraints_manager.global_constraints.append(constraint)

    @property
    def files(self):
        return [e['path'] for e in self.catalog]

    def resolve_source_id(self, source_id):
        return self.catalog.source_id_to_path(source_id)

    def add(self, searchdef, path, allow_global_constraints=True):  # noqa, pylint: disable=W0221
        """
        Add a search definition.

        @param searchdef: a SearchDef or SequenceSearchDef object.
        @param path: path we want to search. this can be a file, dir or glob.
        @param allow_global_constraints: boolean determining whether we want
                                         any global constraints available to be
                                         applied to this path.
        """
        if not allow_global_constraints:
            self.constraints_manager.global_restrictions.add(searchdef.id)

        self.catalog.register(searchdef, path)

    @property
    def num_parallel_tasks(self):
        if self.max_parallel_tasks == 0:
            cpus = 1  # i.e. no parallelism
        else:
            cpus = min(self.max_parallel_tasks, os.cpu_count())

        return min(len(self.files) or 1, cpus)

    @property
    def stats(self):
        """
        Provide stats for the last search run.

        @return: SearchTaskStats object
        """
        return self._stats

    def _get_results(self, results, results_queue, event, stats):
        """
        Collect results from all search task processes.

        @param results: SearchResultsCollection object.
        @param results_queue: results queue used for this search session.
        @param event: event object used to notify this thread to stop.
        @param stats: SearchTaskStats object
        """
        log.debug("fetching results from worker queues")

        while True:
            if not results_queue.empty():
                results.add(results_queue.get())
            elif event.is_set():
                log.debug("exiting results thread")
                break
            else:
                log.debug("total %s results received, %s/%s jobs completed - "
                          "waiting for more", len(results),
                          stats['jobs_completed'], stats['total_jobs'])
                # yield
                time.sleep(0.1)

        log.debug("stopped fetching results (total received=%s)", len(results))

    def _purge_results(self, results, results_queue, expected):
        """
        Purge results from all search task processes.

        @param results: SearchResultsCollection object.
        @param results_queue: results queue used for this search session.
        @param expected: number of results we expect to receive. this is used
                         to do a final sweep once all search tasks are complete
                         to ensure all results have been collected.
        """
        log.debug("purging results (expected=%s)", expected)

        while True:
            if not results_queue.empty():
                results.add(results_queue.get())
            elif expected > len(results):
                try:
                    r = results_queue.get(timeout=RESULTS_QUEUE_TIMEOUT)
                    results.add(r)
                except queue.Empty:
                    log.info("timeout waiting > %s secs to receive results - "
                             "expected=%s, actual=%s", RESULTS_QUEUE_TIMEOUT,
                             expected, len(results))
            else:
                break

        log.debug("stopped purging results (total received=%s)",
                  len(results))

    def _create_results_thread(self, results, results_queue, stats):
        log.debug("creating results queue consumer thread")
        event = threading.Event()
        event.clear()
        t = threading.Thread(target=self._get_results,
                             args=[results, results_queue, event, stats])
        return t, event

    def _stop_results_thread(self, thread, event):
        log.debug("joining/stopping queue consumer thread")
        event.set()
        thread.join()
        log.debug("consumer thread stopped successfully")

    def _ensure_worker_processes_killed(self):
        """
        For some reason it is sometimes possible to for pool termination to
        hang indefinitely because one or more worker process fails to
        terminate. This method ensures that all extant worker child processes
        are killed so that pool termination is guaranteed to complete.
        """
        log.debug("ensuring all pool workers killed")
        worker_pids = []
        for child in multiprocessing.active_children():
            if isinstance(child, multiprocessing.context.ForkProcess):
                if 'ForkProcess' in child.name:
                    worker_pids.append(child.pid)

        ps_out = subprocess.check_output(['ps', '-opid', '--no-headers',
                                          '--ppid',
                                          str(os.getpid())], encoding='utf8')
        child_pids = [int(line.strip()) for line in ps_out.splitlines()]
        log.debug("process has child pids: %s", child_pids)
        for wpid in worker_pids:
            if int(wpid) not in child_pids:
                log.error("worker pid %s no longer a child of this process "
                          "(%s)", wpid, os.getpid())
                continue

            try:
                log.debug('sending SIGKILL to worker process %s', wpid)
                os.kill(wpid, signal.SIGILL)
            except Exception:
                log.debug('worker process %s already killed', wpid)

    def _run_single(self, results_collection, results_store):
        """ Run a single search using this process.

        @param results_collection: SearchResultsCollection object
        @param results_store: ResultsStoreSimple object
        """
        for info in self.catalog:
            task = SearchTask(info,
                              constraints_manager=self.constraints_manager,
                              results_collection=results_collection,
                              results_store=results_store,
                              decode_errors=self.decode_errors)
            self.stats.update(task.execute())

        self.stats['jobs_completed'] = 1
        self.stats['total_jobs'] = 1

    def _run_mp(self, mgr, results, results_store):
        """ Run searches in parallel.

        @param mgr: multiprocessing.Manager object
        @param results: SearchResultsCollection object
        """
        results_queue = mgr.Queue()
        results_thread, event = self._create_results_thread(results,
                                                            results_queue,
                                                            self.stats)
        results_thread_started = False
        try:
            num_workers = self.num_parallel_tasks
            with concurrent.futures.ProcessPoolExecutor(
                                    max_workers=num_workers) as executor:
                jobs = {}
                for info in self.catalog:
                    c_mgr = self.constraints_manager
                    task = SearchTask(info,
                                      constraints_manager=c_mgr,
                                      results_queue=results_queue,
                                      results_store=results_store,
                                      decode_errors=self.decode_errors)
                    job = executor.submit(task.execute)
                    jobs[job] = info['path']
                    self.stats['total_jobs'] += 1

                log.debug("filesearcher: syncing %s job(s)", len(jobs))
                results_thread.start()
                results_thread_started = True
                try:
                    for future in concurrent.futures.as_completed(jobs):
                        self.stats.update(future.result())
                        self.stats['jobs_completed'] += 1
                except concurrent.futures.process.BrokenProcessPool as exc:
                    msg = ("one or more worker processes has died - "
                           "aborting search")
                    raise FileSearchException(msg) from exc

                log.debug("all workers synced")
                # double check nothing is running anymore
                for job, path in jobs.items():
                    log.debug("worker for path '%s' has state: %s", path,
                              repr(job))
                    if job.running():
                        log.info("job for path '%s' still running when "
                                 "not expected to be", path)

                self._stop_results_thread(results_thread, event)
                results_thread = None
                log.debug("purging remaining results (expected=%s, "
                          "remaining=%s)", self.stats['results'],
                          self.stats['results'] - len(results))
                self._purge_results(results, results_queue,
                                    self.stats['results'])

                self._ensure_worker_processes_killed()
                log.debug("terminating pool")
        finally:
            if results_thread is not None and results_thread_started:
                self._stop_results_thread(results_thread, event)

    def run(self):
        """ Run all searches.

        @return: SearchResultsCollection object
        """
        log.debug("filesearcher: starting")
        self.stats.reset()
        if len(self.catalog) == 0:
            log.debug("catalog is empty - nothing to run")
            return SearchResultsCollection(self.catalog, ResultStoreSimple())

        self.stats['searches'] = sum([len(p['searches'])  # noqa, pylint: disable=R1728
                                      for p in self.catalog])
        self.stats['searches_by_job'] = [len(p['searches'])
                                         for p in self.catalog]
        log.debug(repr(self.catalog))
        if len(self.files) > 1:
            log.debug("running searches (parallel=True)")
            with multiprocessing.Manager() as mgr:
                rs = ResultStoreParallel(mgr)
                results = SearchResultsCollection(self.catalog, rs)
                self._run_mp(mgr, results, rs)
                self.stats['parts_deduped'] = rs.parts_deduped
                self.stats['parts_non_deduped'] = rs.parts_non_deduped
                rs.unproxy_results()
        else:
            log.debug("running searches (parallel=False)")
            rs = ResultStoreSimple()
            results = SearchResultsCollection(self.catalog, rs)
            self._run_single(results, rs)
            self.stats['parts_deduped'] = rs.parts_deduped
            self.stats['parts_non_deduped'] = rs.parts_non_deduped

        log.debug("filesearcher: completed (%s)", self.stats)
        return results
